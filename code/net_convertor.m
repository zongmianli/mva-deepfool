function [ net ] = net_convertor()

% Define network CIFAR10-quick
net.layers = {} ;

%% Block 1
% load learnt parameters
load_W1 = load('load_net/W1.mat');
load_b1 = load('load_net/b1.mat');
% Conv1
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{load_W1.W1, load_b1.b1}}, ...% 'weights', {{0.01*randn(5,5,3,32, 'single'), zeros(1, 32, 'single')}}, ...
                           'name', 'conv1', ... % 'learningRate', lr, ...
                           'stride', 1, ...
                           'pad', 0) ;
% Squash1
net.layers{end+1} = struct('type', 'relu', ...
                           'name', 'relu1') ;
% MaxPool1
net.layers{end+1} = struct('type', 'pool', ...
                           'name', 'mpool1', ...
                           'method', 'max', ...
                           'pool', [3 3], ...
                           'stride', 2, ...
                           'pad', [0 1 0 1]) ;
%% Block 2
% load learnt parameters
load_W2 = load('load_net/W2.mat');
load_b2 = load('load_net/b2.mat');
% Conv2
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{load_W2.W2, load_b2.b2}}, ...%'weights', {{0.05*randn(5,5,32,32, 'single'), zeros(1,32,'single')}}, ...
                           'name', 'conv2', ... %'learningRate', lr, ...
                           'stride', 1, ...
                           'pad', 0) ;
% Squash2
net.layers{end+1} = struct('type', 'relu', ...
                           'name', 'relu2') ;
% MaxPool2
net.layers{end+1} = struct('type', 'pool', ...
                           'name', 'mpool2', ...
                           'method', 'max', ...
                           'pool', [3 3], ...
                           'stride', 2, ...
                           'pad', [0 1 0 1]) ; % Emulate caffe

%% Block 3
% load learnt parameters
load_W3 = load('load_net/W3.mat');
load_W3.W3 = reshape(load_W3.W3, [1,1,size(load_W3.W3)]);
load_b3 = load('load_net/b3.mat');
% Dense3
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{load_W3.W3, load_b3.b3}}, ... %'weights', {{0.05*randn(5,5,32,64, 'single'), zeros(1,64,'single')}}, ...
                           'name', 'fc3', ...%'learningRate', lr, ...
                           'stride', 1, ...
                           'pad', 0) ;
% Squash3
net.layers{end+1} = struct('type', 'relu', ...
                           'name', 'relu3') ;
% net.layers{end+1} = struct('type', 'pool', ...
%                            'method', 'avg', ...
%                            'pool', [3 3], ...
%                            'stride', 2, ...
%                            'pad', [0 1 0 1]) ; % Emulate caffe

%% Block 4
% load learnt parameters
load_W4 = load('load_net/W4.mat');
load_W4.W4 = reshape(load_W4.W4, [1,1,size(load_W4.W4)]);
load_b4 = load('load_net/b4.mat');
% Dense4
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{load_W4.W4, load_b4.b4}}, ... 
                           'name', 'fc4', ...
                           'stride', 1, ...
                           'pad', 0) ;
% Squash4
net.layers{end+1} = struct('type', 'relu', ...
                           'name', 'relu4') ;

%% Block 5
% load learnt parameters
load_W5 = load('load_net/W5.mat');
load_W5.W5 = reshape(load_W5.W5, [1,1,size(load_W5.W5)]);
load_b5 = load('load_net/b5.mat');
% Dense5
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{load_W5.W5, load_b4.b4}}, ... 
                           'name', 'fc5', ...
                           'stride', 1, ...
                           'pad', 0) ;

%% Loss layer
net.layers{end+1} = struct('type', 'softmaxloss',...
                           'name','softmax5') ;

% Meta parameters
net.meta.inputSize = [64 64 3] ;
% net.meta.trainOpts.learningRate = [0.05*ones(1,30) 0.005*ones(1,10) 0.0005*ones(1,5)] ;
% net.meta.trainOpts.weightDecay = 0.0001 ;
% net.meta.trainOpts.batchSize = 100 ;
% net.meta.trainOpts.numEpochs = numel(net.meta.trainOpts.learningRate) ;

net = vl_simplenn_tidy(net); % add compatibility to newer versions of MatConvNet
end

